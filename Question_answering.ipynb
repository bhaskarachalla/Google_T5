{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL6ux_mUO6Ws",
        "outputId": "04c6f999-286e-45ed-9f7c-41a1b64996e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.12.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import PyPDF2"
      ],
      "metadata": {
        "id": "A-4bcFVufVeq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "07lvQIroO6Wu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, questions_answers, tokenizer, max_length=512):\n",
        "        self.questions_answers = questions_answers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions_answers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.questions_answers[idx]\n",
        "\n",
        "        # Preprocess input and output for the model\n",
        "        input_text = f\"question: {question} context: {pdf_text}\"\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encoding = self.tokenizer(\n",
        "            answer, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Return input_ids and labels (target)\n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].flatten(),  # Flatten the tensor\n",
        "            'attention_mask': input_encoding['attention_mask'].flatten(),  # Flatten the tensor\n",
        "            'labels': target_encoding['input_ids'].flatten()  # Flatten the tensor\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8DqXctffi8",
        "outputId": "e0854f5c-f247-41e3-a155-6a0c8fc9a083"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract text from the PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Load your PDF file\n",
        "pdf_path = 'physics_for_beginners-_PDf(copy).pdf'\n",
        "pdf_text = extract_text_from_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "O43LDxslfiH7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define your set of questions and answers\n",
        "def prepare_questions_answers(pdf_text):\n",
        "    # Example set of questions based on the PDF content; adjust as necessary\n",
        "    questions_answers = [\n",
        "        {\n",
        "        \"query\": \"What is the definition of physics?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Physics is the science that deals with matter, energy, motion, and force. It studies the fundamental building blocks of the universe and how they interact.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is an inertial frame of reference according to Newton's laws?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"An inertial frame of reference is one in which Newton's laws of motion are valid, meaning that objects in motion stay in motion unless acted upon by an external force. This frame does not accelerate and moves uniformly in a straight line.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How did Roemer estimate the speed of light?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Roemer estimated the speed of light by studying the eclipses of Jupiter’s satellites and noticing that the time between eclipses varied depending on the distance between Earth and Jupiter.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the interference pattern of waves?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"When two identical waves overlap, they can interfere constructively, creating points of maximum displacement, or destructively, canceling each other out at points of zero displacement.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How does special relativity redefine space and time?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Special relativity views space and time as a single four-dimensional entity known as 'space-time,' where measurements of time and distance are relative to the observer’s motion.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the cosmic speed limit in Einstein’s mechanics?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"The cosmic speed limit is the speed of light (c). No object can be accelerated to exceed the speed of light, as mass increases infinitely as it approaches c.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How does general relativity differ from special relativity?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"While special relativity deals with non-accelerating frames of reference, general relativity extends the theory to include gravity and acceleration, describing gravity as the curvature of space-time caused by mass.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What does the Heisenberg Uncertainty Principle state?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"The Heisenberg Uncertainty Principle states that it is impossible to know both the position and momentum of a particle with absolute certainty. The more precisely one is known, the less precisely the other can be known.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the wave-particle duality concept proposed by de Broglie?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"De Broglie proposed that particles, such as electrons, can behave as waves, similar to how light, traditionally considered a wave, can exhibit particle-like properties.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the Copenhagen Interpretation of quantum mechanics?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"The Copenhagen Interpretation, mainly attributed to Niels Bohr, suggests that particles exist in all possible states until they are observed, at which point the wave function collapses into one observable state.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"According to Einstein’s theories, how do different observers measure events differently?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"In relativity, observers moving at different velocities will measure different times and distances for the same events, and they may disagree about the simultaneity or order of events.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is Richard Feynman’s 'sum-over-histories' approach?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Feynman’s 'sum-over-histories' approach suggests that particles, when moving from point A to point B, take into account every possible path, and the overall outcome is a sum of all these possible paths.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How is average acceleration calculated?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Average acceleration is the change in velocity divided by the time interval during which the change occurs.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the principle of energy conservation?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"The principle of energy conservation states that energy cannot be created or destroyed, only converted from one form to another.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How does light behave when it strikes a polished surface?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"When light strikes a polished surface, it is reflected, similar to a billiard ball bouncing off the side of a pool table.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the mass-energy equivalence as described by Einstein?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"The mass-energy equivalence is described by Einstein's famous equation E = mc^2, which states that mass and energy are interchangeable.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What did J.J. Thomson discover using cathode rays?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"J.J. Thomson discovered the electron as a negatively charged particle by studying the behavior of cathode rays under electric and magnetic fields.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is dark matter?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Dark matter is a hypothetical form of matter that does not emit or interact with electromagnetic radiation, making it invisible, but it is believed to constitute most of the matter in the universe.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What discovery was made about the expansion of the universe in 1998?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"In 1998, astronomers discovered that the expansion of the universe is accelerating, a finding attributed to the influence of dark energy.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What dual nature does a photon exhibit?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"A photon exhibits both wave-like and particle-like properties, a phenomenon known as wave-particle duality.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the goal of string theory in physics?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"String theory aims to unify general relativity and quantum mechanics by proposing that the fundamental constituents of reality are one-dimensional strings, not point-like particles.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What challenge did Einstein pursue in his later years?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Einstein spent the last decades of his life searching for a unified field theory that could describe all fundamental forces within a single framework.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What happens when two waves meet in phase?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"When two waves meet in phase, their crests and troughs reinforce each other, creating a wave of greater amplitude.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How is temperature related to the energy of particles?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Temperature is a measure of the average kinetic energy of the particles in a substance. Higher temperatures correspond to higher particle motion.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is a standing wave?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"A standing wave is a wave that oscillates in place without traveling, with points of maximum and minimum amplitude remaining fixed.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How does probability play a role in quantum mechanics?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"In quantum mechanics, the behavior of particles is described probabilistically, and the exact outcome of an event cannot be predicted, only the likelihood of different outcomes.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What does Planck’s law state about energy?\",\n",
        "        #\"context\": pdf_text,\n",
        "        \"answer\": \"Planck’s law states that energy is quantized and can only be emitted or absorbed in discrete amounts, called quanta.\"\n",
        "    }\n",
        "    ]\n",
        "    return questions_answers\n",
        "\n",
        "questions_answers = prepare_questions_answers(pdf_text)"
      ],
      "metadata": {
        "id": "yJqz1YWPflVh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RYGnKRN7O6Ww"
      },
      "outputs": [],
      "source": [
        "# Step 5: Create the dataset and DataLoader\n",
        "train_dataset = TextDataset(questions_answers, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tw3w-qKkO6Ww"
      },
      "outputs": [],
      "source": [
        "# Step 6: Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbCyDFWMO6Ww",
        "outputId": "cd46150a-5198-4f03-a98e-1b29b22220f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "m5bvo09VO6Ww",
        "outputId": "5ab29d70-e148-4298-a561-c93c52b00784"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 03:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=42, training_loss=3.5777769542875744, metrics={'train_runtime': 182.9338, 'train_samples_per_second': 0.443, 'train_steps_per_second': 0.23, 'total_flos': 49325589135360.0, 'train_loss': 3.5777769542875744, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Step 8: Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save the fine-tuned model\n",
        "trainer.save_model('./fine_tuned_model')"
      ],
      "metadata": {
        "id": "XlW9WNNVR9T4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Load fine-tuned model for inference\n",
        "model = T5ForConditionalGeneration.from_pretrained('./fine_tuned_model')\n",
        "\n",
        "# Load the tokenizer from the original pre-trained model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base') # Use the appropriate base model name if it's different"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZheSOu0RuN1",
        "outputId": "6c32cc0e-2d58-41ea-efa8-d8df9d5b760d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Function to answer questions based on context\n",
        "def get_answer(question, context, model, tokenizer):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids)\n",
        "\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Example usage with a specific question\n",
        "example_question = \"What is the principle of conservation of energy?\"\n",
        "context = \"The principle states that energy cannot be created or destroyed, only transformed from one form to another.\"  # Adjust the context as needed\n",
        "answer = get_answer(example_question, context, model, tokenizer)\n",
        "\n",
        "# Print the final answer only\n",
        "print(f\"Final Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqYG3COnX0xH",
        "outputId": "c9883ab2-e3c6-487d-efa8-b3ba15ca8426"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer: that energy cannot be created or destroyed, only transformed from one form to another\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Function to answer questions based on context\n",
        "def get_answer(question, context, model, tokenizer):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids)\n",
        "\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Test with a list of questions\n",
        "test_questions = [\n",
        "    \"What is the principle of conservation of energy?\",\n",
        "    \"What is discussed in the line about Newton's second law?\",\n",
        "]\n",
        "\n",
        "# Iterate through the test questions and get answers\n",
        "for question in test_questions:\n",
        "    answer = get_answer(question, pdf_text, model, tokenizer)  # Use pdf_text as context\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")\n"
      ],
      "metadata": {
        "id": "N1rv7cWkiGVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}